import numpy as np
import pandas as pd
# from sklearn import preprocessing
# import math
import os




# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/
# %cd /content/drive/MyDrive/Machine\ Learning\ Practice/
# %cd Linear Regression/
df = pd.read_csv('train.csv')

y = df['Survived']

df.loc[:,['Pclass']]=df.loc[:,['Pclass']].astype('object')

df['Embarked'].fillna("S", inplace=True)

df['Age'].fillna(df['Age'].mean(), inplace=True)
df.isnull().sum()

Sexdict = {'male': 0, 'female': 1}
df['Sex_binary']=df['Sex'].map(Sexdict)
df.pop('Sex')
df.head()

df = pd.concat([df,pd.get_dummies(df['Pclass'], prefix='Pclass')], axis =1)
df = pd.concat([df,pd.get_dummies(df['Embarked'], prefix='Embarked')], axis =1)
df.pop('Pclass')
df.pop('Embarked')
df.head()

features =df[['Age','SibSp','Parch','Fare','Sex_binary','Pclass_1','Pclass_2','Pclass_3','Embarked_C','Embarked_Q','Embarked_S']]
features.describe()

# scaler = preprocessing.StandardScaler().fit(features.values)
# features_scaled = scaler.transform(features.values)
# new_features=pd.DataFrame(features_scaled)
# new_features.describe()

def norm_x(column, max, min):
  column_array = column.values
  for i in range(len(column_array)):
    column_array[i] = (column_array[i] - min)/(max - min)
  return pd.Series(column_array)

random_var0= norm_x(features['Age'], features['Age'].max(), features['Age'].min())
features.replace(features['Age'],random_var0)

random_var1= norm_x(features['SibSp'], features['SibSp'].max(), features['SibSp'].min())
features.replace(features['SibSp'],random_var1)

random_var2= norm_x(features['Parch'], features['Parch'].max(), features['Parch'].min())
features.replace(features['Parch'],random_var2)

random_var3= norm_x(features['Fare'], features['Fare'].max(), features['Fare'].min())
features.replace(features['Fare'],random_var3)

def activation(z):
  return 1/(1 + np.exp(-z))

def gradient_descent(x, y, theta,alpha, m, h1):
    # h_ = activation(np.dot(x,theta))
    grad = np.dot((h1 - y), x) / m
    theta = theta - alpha*grad
    return theta

def loss(x, y, theta, m,h):
  loss1 = np.dot(np.log(h),y)
  loss2 = np.dot((np.log(1-h)), (1-y))
  loss_out = -1*(loss1+ loss2)/m
  return loss_out

x = features.values
m = x.shape[0]
bias = np.ones([m,1])
x = np.hstack((bias, x))
theta = np.zeros(x.shape[1])

y_hat = activation(np.dot(x,theta))
y = y.values
alpha = 0.005

theta_out = gradient_descent(x, y, theta, alpha, m, y_hat)
for i in range(10000):
  h1 = activation(np.dot(x,theta_out))
  theta_out = gradient_descent(x, y, theta_out, alpha, m, h1)
  loss_out = loss(x, y, theta_out, m,h1)
  if i%500==0:
    print(loss_out)
  if i ==10000:
    theta_final = theta_out



